{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786b4ce7",
   "metadata": {},
   "source": [
    "# Bipartite Model for Concept Bottleneck\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load and prepare data with rich concept annotations\n",
    "2. Define concept and task metadata with distributions and cardinalities\n",
    "3. Build a BipartiteModel that automatically constructs a PGM\n",
    "4. Use Propagators to create encoder and predictor factors\n",
    "5. Train the model with concept and task supervision\n",
    "6. Apply interventions within the BipartiteModel framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90380c26",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "We import the necessary libraries:\n",
    "- **PyTorch**: for neural network building blocks and distributions\n",
    "- **sklearn**: for evaluation metrics\n",
    "- **torch_concepts**: for Annotations, BipartiteModel, Propagators, and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.distributions import RelaxedOneHotCategorical, RelaxedBernoulli\n",
    "\n",
    "from torch_concepts import Annotations, AxisAnnotation\n",
    "from torch_concepts.data import ToyDataset\n",
    "from torch_concepts.nn import (\n",
    "    ProbEncoderFromEmb, \n",
    "    ProbPredictor, \n",
    "    RandomPolicy, \n",
    "    DoIntervention, \n",
    "    intervention, \n",
    "    DeterministicInference, \n",
    "    BipartiteModel, \n",
    "    Propagator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e90e6",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "We load the XOR toy dataset and prepare the training data:\n",
    "- **Features (x_train)**: input features for the model\n",
    "- **Concepts (c_train)**: intermediate concept labels (binary: c1, c2)\n",
    "- **Targets (y_train)**: task labels (converted to one-hot encoding with 2 classes)\n",
    "- **Names**: concept and task attribute names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f985983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dims = 10\n",
    "n_epochs = 500\n",
    "n_samples = 1000\n",
    "concept_reg = 0.5\n",
    "\n",
    "# Load toy XOR dataset\n",
    "data = ToyDataset('xor', size=n_samples, random_state=42)\n",
    "x_train = data.data\n",
    "c_train = data.concept_labels\n",
    "y_train = data.target_labels\n",
    "concept_names_raw = data.concept_attr_names\n",
    "task_names_raw = data.task_attr_names\n",
    "\n",
    "# Convert y_train to one-hot encoding (2 classes)\n",
    "y_train = torch.cat([y_train, 1 - y_train], dim=1)\n",
    "\n",
    "# Define concept and task names for the model\n",
    "concept_names = ('c1', 'c2')\n",
    "task_names = ('xor',)\n",
    "\n",
    "print(f\"Dataset loaded:\")\n",
    "print(f\"  Features shape: {x_train.shape}\")\n",
    "print(f\"  Concepts shape: {c_train.shape}\")\n",
    "print(f\"  Targets shape: {y_train.shape}\")\n",
    "print(f\"  Concept names: {concept_names}\")\n",
    "print(f\"  Task names: {task_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768f1da",
   "metadata": {},
   "source": [
    "## 3. Rich Annotations with Metadata\n",
    "\n",
    "The **Annotations** object in the BipartiteModel framework supports rich metadata:\n",
    "- **Cardinalities**: The number of classes/dimensions for each variable\n",
    "- **Metadata**: Additional information for each variable including:\n",
    "  - **distribution**: The probability distribution type\n",
    "  - **type**: Variable type (e.g., 'binary', 'categorical')\n",
    "  - **description**: Human-readable description\n",
    "\n",
    "This metadata is used by the BipartiteModel to automatically:\n",
    "- Create appropriate Variables\n",
    "- Set up correct probability distributions\n",
    "- Configure the PGM structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ba76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cardinalities (number of classes for each variable)\n",
    "cardinalities = (1, 1, 2)  # c1: 1 (binary), c2: 1 (binary), xor: 2 (one-hot)\n",
    "\n",
    "# Define metadata for each variable\n",
    "metadata = {\n",
    "    'c1': {\n",
    "        'distribution': RelaxedBernoulli, \n",
    "        'type': 'binary', \n",
    "        'description': 'Concept 1'\n",
    "    },\n",
    "    'c2': {\n",
    "        'distribution': RelaxedBernoulli, \n",
    "        'type': 'binary', \n",
    "        'description': 'Concept 2'\n",
    "    },\n",
    "    'xor': {\n",
    "        'distribution': RelaxedOneHotCategorical, \n",
    "        'type': 'binary', \n",
    "        'description': 'XOR Task'\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create rich annotations\n",
    "annotations = Annotations({\n",
    "    1: AxisAnnotation(\n",
    "        concept_names + task_names, \n",
    "        cardinalities=cardinalities, \n",
    "        metadata=metadata\n",
    "    )\n",
    "})\n",
    "\n",
    "print(\"Annotations structure:\")\n",
    "print(f\"  Variables: {concept_names + task_names}\")\n",
    "print(f\"  Cardinalities: {cardinalities}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "for name, meta in metadata.items():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Distribution: {meta['distribution'].__name__}\")\n",
    "    print(f\"    Type: {meta['type']}\")\n",
    "    print(f\"    Description: {meta['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109f17d",
   "metadata": {},
   "source": [
    "## 4. BipartiteModel: High-Level Model Construction\n",
    "\n",
    "The **BipartiteModel** is a high-level abstraction that:\n",
    "- Automatically constructs a PGM from annotations\n",
    "- Uses **Propagators** to create encoder and predictor factors\n",
    "- Manages the bipartite structure: concepts → tasks\n",
    "- Exposes the underlying PGM for inference and interventions\n",
    "\n",
    "### Propagators:\n",
    "- **Propagator(ProbEncoderFromEmb)**: Creates encoder factors for concepts\n",
    "- **Propagator(ProbPredictor)**: Creates predictor factors for tasks\n",
    "\n",
    "The BipartiteModel automatically:\n",
    "1. Creates Variables from annotations\n",
    "2. Builds Factors using Propagators\n",
    "3. Constructs the PGM with proper dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder (input features -> embedding)\n",
    "encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x_train.shape[1], latent_dims), \n",
    "    torch.nn.LeakyReLU()\n",
    ")\n",
    "\n",
    "# Create the BipartiteModel\n",
    "concept_model = BipartiteModel(\n",
    "    task_names=task_names,\n",
    "    latent_dims=latent_dims,\n",
    "    annotations=annotations,\n",
    "    concept_propagator=Propagator(ProbEncoderFromEmb),\n",
    "    task_propagator=Propagator(ProbPredictor)\n",
    ")\n",
    "\n",
    "print(\"BipartiteModel structure:\")\n",
    "print(f\"  Task names: {task_names}\")\n",
    "print(f\"  Latent dimensions: {latent_dims}\")\n",
    "print(f\"  Concept propagator: {ProbEncoderFromEmb.__name__}\")\n",
    "print(f\"  Task propagator: {ProbPredictor.__name__}\")\n",
    "print(f\"\\nUnderlying PGM:\")\n",
    "print(concept_model.pgm)\n",
    "print(f\"\\nThe model automatically created:\")\n",
    "print(f\"  - Variables for concepts and tasks\")\n",
    "print(f\"  - Encoder factors (embedding → concepts)\")\n",
    "print(f\"  - Predictor factors (concepts → tasks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2117604",
   "metadata": {},
   "source": [
    "## 5. Inference Engine\n",
    "\n",
    "We use the **DeterministicInference** engine on the BipartiteModel's underlying PGM:\n",
    "- **Evidence**: The embedding computed from input features\n",
    "- **Query**: The concepts and tasks we want to infer\n",
    "\n",
    "The BipartiteModel exposes its PGM via the `.pgm` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb637558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the inference engine with the BipartiteModel's PGM\n",
    "inference_engine = DeterministicInference(concept_model.pgm)\n",
    "\n",
    "# Define the query (what we want to infer)\n",
    "query_concepts = [\"c1\", \"c2\", \"xor\"]\n",
    "\n",
    "print(\"Inference setup:\")\n",
    "print(f\"  Engine: DeterministicInference\")\n",
    "print(f\"  PGM source: concept_model.pgm\")\n",
    "print(f\"  Query variables: {query_concepts}\")\n",
    "print(f\"\\nInference flow:\")\n",
    "print(f\"  x_train → encoder → embedding → [c1, c2] → xor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779aecb3",
   "metadata": {},
   "source": [
    "## 6. Complete Model Pipeline\n",
    "\n",
    "We combine the encoder and BipartiteModel into a complete pipeline:\n",
    "- **encoder**: Maps input features to latent embedding\n",
    "- **concept_model**: BipartiteModel that maps embedding to concepts and tasks\n",
    "\n",
    "This creates a Sequential model for easy training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine encoder and concept_model into a Sequential pipeline\n",
    "model = torch.nn.Sequential(encoder, concept_model)\n",
    "\n",
    "print(\"Complete model pipeline:\")\n",
    "print(model)\n",
    "print(f\"\\nPipeline structure:\")\n",
    "print(f\"  1. Encoder: {x_train.shape[1]} features → {latent_dims} dimensions\")\n",
    "print(f\"  2. BipartiteModel: {latent_dims} dimensions → concepts & tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054aa980",
   "metadata": {},
   "source": [
    "## 7. Training\n",
    "\n",
    "We train the complete model with a combined loss:\n",
    "- **Concept loss**: BCE loss between predicted and true concept labels (c1, c2)\n",
    "- **Task loss**: BCE loss between predicted and true task labels (xor)\n",
    "- **Total loss**: `concept_loss + concept_reg * task_loss`\n",
    "\n",
    "Training process:\n",
    "1. Compute embedding from input features\n",
    "2. Query the inference engine with the embedding as evidence\n",
    "3. Split predictions into concepts and tasks\n",
    "4. Compute losses and backpropagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46cab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute embedding\n",
    "    emb = encoder(x_train)\n",
    "    \n",
    "    # Inference: query the PGM with embedding as evidence\n",
    "    cy_pred = inference_engine.query(query_concepts, evidence={'embedding': emb})\n",
    "    \n",
    "    # Split predictions: first columns are concepts, remaining are task\n",
    "    c_pred = cy_pred[:, :c_train.shape[1]]\n",
    "    y_pred = cy_pred[:, c_train.shape[1]:]\n",
    "\n",
    "    # Compute loss\n",
    "    concept_loss = loss_fn(c_pred, c_train)\n",
    "    task_loss = loss_fn(y_pred, y_train)\n",
    "    loss = concept_loss + concept_reg * task_loss\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log progress\n",
    "    if epoch % 100 == 0:\n",
    "        task_accuracy = accuracy_score(y_train, y_pred > 0.)\n",
    "        concept_accuracy = accuracy_score(c_train, c_pred > 0.)\n",
    "        print(f\"Epoch {epoch}: Loss {loss.item():.2f} | Task Acc: {task_accuracy:.2f} | Concept Acc: {concept_accuracy:.2f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc77ae8",
   "metadata": {},
   "source": [
    "## 8. Baseline Predictions (No Intervention)\n",
    "\n",
    "Let's examine the model's predictions without any interventions.\n",
    "The output contains concatenated predictions: [c1, c2, xor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    emb = encoder(x_train)\n",
    "    cy_pred = inference_engine.query(query_concepts, evidence={'embedding': emb})\n",
    "\n",
    "print(\"Baseline predictions (first 5 samples):\")\n",
    "print(\"Format: [c1, c2, xor_class0, xor_class1]\")\n",
    "print(cy_pred[:5])\n",
    "print(f\"\\nShape: {cy_pred.shape}\")\n",
    "print(f\"  Columns 0-1: concept predictions (c1, c2)\")\n",
    "print(f\"  Columns 2-3: task predictions (xor one-hot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd5cfd0",
   "metadata": {},
   "source": [
    "## 9. Interventions in BipartiteModel\n",
    "\n",
    "The BipartiteModel framework supports interventions on the underlying PGM:\n",
    "- Access the PGM's factor modules via `concept_model.pgm.factor_modules`\n",
    "- Apply interventions to specific factors (e.g., \"c1.encoder\")\n",
    "- Effects propagate through the graph structure\n",
    "\n",
    "### Intervention Setup:\n",
    "- **Policy**: RandomPolicy to randomly select samples and intervene on concept c1\n",
    "- **Strategy**: DoIntervention to set c1 to a constant value (-10)\n",
    "- **Layer**: Intervene at the \"c1.encoder\" factor in the PGM\n",
    "- **Quantile**: 1.0 (intervene on all selected samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66dba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embedding for intervention\n",
    "emb = encoder(x_train)\n",
    "\n",
    "# Create annotations for intervention\n",
    "c_annotations = Annotations({1: AxisAnnotation([\"c1\"])})\n",
    "\n",
    "# Define intervention policy and strategy\n",
    "int_policy_c = RandomPolicy(\n",
    "    out_annotations=c_annotations, \n",
    "    scale=100, \n",
    "    subset=[\"c1\"]\n",
    ")\n",
    "int_strategy_c = DoIntervention(\n",
    "    model=concept_model.pgm.factor_modules, \n",
    "    constants=-10\n",
    ")\n",
    "\n",
    "print(\"Intervention configuration:\")\n",
    "print(f\"  Policy: RandomPolicy on concept 'c1'\")\n",
    "print(f\"  Strategy: DoIntervention with constant value -10\")\n",
    "print(f\"  Target layer: c1.encoder (in BipartiteModel's PGM)\")\n",
    "print(f\"  Quantile: 1.0 (intervene on all selected samples)\")\n",
    "print(f\"\\nThis intervention will:\")\n",
    "print(f\"  1. Randomly select samples\")\n",
    "print(f\"  2. Set concept c1 to -10 for those samples\")\n",
    "print(f\"  3. Propagate the effect through the BipartiteModel to xor prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9897f20",
   "metadata": {},
   "source": [
    "## 10. Applying the Intervention\n",
    "\n",
    "Now we apply the intervention and observe how the predictions change.\n",
    "Compare these results with the baseline predictions above to see the intervention's effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions with intervention:\")\n",
    "with intervention(\n",
    "    policies=[int_policy_c],\n",
    "    strategies=[int_strategy_c],\n",
    "    on_layers=[\"c1.encoder\"],\n",
    "    quantiles=[1]\n",
    "):\n",
    "    cy_pred_intervened = inference_engine.query(query_concepts, evidence={'embedding': emb})\n",
    "    print(\"Format: [c1, c2, xor_class0, xor_class1]\")\n",
    "    print(cy_pred_intervened[:5])\n",
    "\n",
    "print(\"\\nNote: Compare with baseline predictions above.\")\n",
    "print(\"You should see c1 values changed to -10 for randomly selected samples,\")\n",
    "print(\"and corresponding changes in the xor predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675f06a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored the BipartiteModel framework for concept-based learning:\n",
    "\n",
    "1. **Data**: Loaded the XOR toy dataset with binary concepts\n",
    "2. **Rich Annotations**: Defined metadata including distributions, types, and descriptions\n",
    "3. **BipartiteModel**: High-level abstraction that automatically builds a PGM\n",
    "4. **Propagators**: Used to create encoder and predictor factors automatically\n",
    "5. **Inference**: Queried the underlying PGM for predictions\n",
    "6. **Training**: Trained with combined concept and task supervision\n",
    "7. **Interventions**: Applied causal interventions via the PGM structure\n",
    "\n",
    "### Key Advantages of BipartiteModel:\n",
    "- **High-level abstraction**: Simplified PGM construction from annotations\n",
    "- **Automatic structure**: Model builds Variables and Factors automatically\n",
    "- **Rich metadata**: Support for distributions, cardinalities, and descriptions\n",
    "- **Propagators**: Flexible way to specify encoder/predictor architectures\n",
    "- **PGM access**: Full access to underlying PGM for advanced operations\n",
    "- **Less boilerplate**: Reduces code needed compared to manual PGM construction\n",
    "\n",
    "### Comparison with Other Approaches:\n",
    "- **vs. Layer-based**: More structured, explicit graph representation\n",
    "- **vs. Manual PGM**: Less code, automatic construction from metadata\n",
    "- **Best for**: Production systems, complex models with many concepts/tasks\n",
    "\n",
    "This framework is ideal for:\n",
    "- Large-scale concept-based models with many variables\n",
    "- Systems requiring rich metadata for interpretability\n",
    "- Applications needing both ease-of-use and flexibility\n",
    "- Production deployments with complex concept hierarchies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptarium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
