{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786b4ce7",
   "metadata": {},
   "source": [
    "# Bipartite Model for Concept Bottleneck\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load and prepare data with rich concept annotations\n",
    "2. Define concept and task metadata with distributions and cardinalities\n",
    "3. Build a BipartiteModel that automatically constructs a ProbabilisticModel\n",
    "4. Use LazyConstructors to create encoder and predictor factors\n",
    "5. Train the model with concept and task supervision\n",
    "6. Apply interventions within the BipartiteModel framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90380c26",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "We import the necessary libraries:\n",
    "- **PyTorch**: for neural network building blocks and distributions\n",
    "- **sklearn**: for evaluation metrics\n",
    "- **torch_concepts**: for Annotations, BipartiteModel, LazyConstructors, and inference"
   ]
  },
  {
   "cell_type": "code",
   "id": "d84fa865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:03:55.399478Z",
     "start_time": "2025-11-14T10:03:55.395846Z"
    }
   },
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.distributions import RelaxedOneHotCategorical, RelaxedBernoulli\n",
    "\n",
    "from torch_concepts import Annotations, AxisAnnotation\n",
    "from torch_concepts.data import ToyDataset\n",
    "from torch_concepts.nn import (\n",
    "    LinearZC,\n",
    "    LinearCC,\n",
    "    RandomPolicy,\n",
    "    DoIntervention,\n",
    "    intervention,\n",
    "    DeterministicInference,\n",
    "    BipartiteModel,\n",
    "    LazyConstructor, UniformPolicy\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "e08e90e6",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "We load the XOR toy dataset and prepare the training data:\n",
    "- **Features (x_train)**: input features for the model\n",
    "- **Concepts (c_train)**: intermediate concept labels (binary: c1, c2)\n",
    "- **Targets (y_train)**: task labels (converted to one-hot encoding with 2 classes)\n",
    "- **Names**: concept and task attribute names"
   ]
  },
  {
   "cell_type": "code",
   "id": "f985983d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:03:55.409901Z",
     "start_time": "2025-11-14T10:03:55.405613Z"
    }
   },
   "source": [
    "# Hyperparameters\n",
    "latent_dims = 10\n",
    "n_epochs = 500\n",
    "n_samples = 1000\n",
    "concept_reg = 0.5\n",
    "\n",
    "# Load toy XOR dataset\n",
    "data = ToyDataset('xor', size=n_samples, random_state=42)\n",
    "x_train = data.data\n",
    "c_train = data.concept_labels\n",
    "y_train = data.target_labels\n",
    "concept_names_raw = data.concept_attr_names\n",
    "task_names_raw = data.task_attr_names\n",
    "\n",
    "# Convert y_train to one-hot encoding (2 classes)\n",
    "y_train = torch.cat([y_train, 1 - y_train], dim=1)\n",
    "\n",
    "# Define concept and task names for the model\n",
    "concept_names = ('c1', 'c2')\n",
    "task_names = ('xor',)\n",
    "\n",
    "print(f\"Dataset loaded:\")\n",
    "print(f\"  Features shape: {x_train.shape}\")\n",
    "print(f\"  Concepts shape: {c_train.shape}\")\n",
    "print(f\"  Targets shape: {y_train.shape}\")\n",
    "print(f\"  Concept names: {concept_names}\")\n",
    "print(f\"  Task names: {task_names}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded:\n",
      "  Features shape: torch.Size([1000, 2])\n",
      "  Concepts shape: torch.Size([1000, 2])\n",
      "  Targets shape: torch.Size([1000, 2])\n",
      "  Concept names: ('c1', 'c2')\n",
      "  Task names: ('xor',)\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "d768f1da",
   "metadata": {},
   "source": [
    "## 3. Rich Annotations with Metadata\n",
    "\n",
    "The **Annotations** object in the BipartiteModel framework supports rich metadata:\n",
    "- **Cardinalities**: The number of classes/dimensions for each variable\n",
    "- **Metadata**: Additional information for each variable including:\n",
    "  - **distribution**: The probability distribution type\n",
    "  - **type**: Variable type (e.g., 'binary', 'categorical')\n",
    "  - **description**: Human-readable description\n",
    "\n",
    "This metadata is used by the BipartiteModel to automatically:\n",
    "- Create appropriate Variables\n",
    "- Set up correct probability distributions\n",
    "- Configure the ProbabilisticModel structure"
   ]
  },
  {
   "cell_type": "code",
   "id": "286ba76a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:03:55.431598Z",
     "start_time": "2025-11-14T10:03:55.428004Z"
    }
   },
   "source": [
    "# Define cardinalities (number of classes for each variable)\n",
    "cardinalities = (1, 1, 2)  # c1: 1 (binary), c2: 1 (binary), xor: 2 (one-hot)\n",
    "\n",
    "# Define metadata for each variable\n",
    "metadata = {\n",
    "    'c1': {\n",
    "        'distribution': RelaxedBernoulli, \n",
    "        'type': 'binary', \n",
    "        'description': 'Concept 1'\n",
    "    },\n",
    "    'c2': {\n",
    "        'distribution': RelaxedBernoulli, \n",
    "        'type': 'binary', \n",
    "        'description': 'Concept 2'\n",
    "    },\n",
    "    'xor': {\n",
    "        'distribution': RelaxedOneHotCategorical, \n",
    "        'type': 'binary', \n",
    "        'description': 'XOR Task'\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create rich annotations\n",
    "annotations = Annotations({\n",
    "    1: AxisAnnotation(\n",
    "        concept_names + task_names, \n",
    "        cardinalities=cardinalities, \n",
    "        metadata=metadata\n",
    "    )\n",
    "})\n",
    "\n",
    "print(\"Annotations structure:\")\n",
    "print(f\"  Variables: {concept_names + task_names}\")\n",
    "print(f\"  Cardinalities: {cardinalities}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "for name, meta in metadata.items():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Distribution: {meta['distribution'].__name__}\")\n",
    "    print(f\"    Type: {meta['type']}\")\n",
    "    print(f\"    Description: {meta['description']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations structure:\n",
      "  Variables: ('c1', 'c2', 'xor')\n",
      "  Cardinalities: (1, 1, 2)\n",
      "\n",
      "Metadata:\n",
      "  c1:\n",
      "    Distribution: RelaxedBernoulli\n",
      "    Type: binary\n",
      "    Description: Concept 1\n",
      "  c2:\n",
      "    Distribution: RelaxedBernoulli\n",
      "    Type: binary\n",
      "    Description: Concept 2\n",
      "  xor:\n",
      "    Distribution: RelaxedOneHotCategorical\n",
      "    Type: binary\n",
      "    Description: XOR Task\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "3109f17d",
   "metadata": {},
   "source": [
    "## 4. BipartiteModel: High-Level Model Construction\n",
    "\n",
    "The **BipartiteModel** is a high-level abstraction that:\n",
    "- Automatically constructs a ProbabilisticModel from annotations\n",
    "- Uses **LazyConstructors** to create encoder and predictor factors\n",
    "- Manages the bipartite structure: concepts → tasks\n",
    "- Exposes the underlying ProbabilisticModel for inference and interventions\n",
    "\n",
    "### LazyConstructors:\n",
    "- **LazyConstructor(LinearZC)**: Creates encoder factors for concepts\n",
    "- **LazyConstructor(LinearCC)**: Creates predictor factors for tasks\n",
    "\n",
    "The BipartiteModel automatically:\n",
    "1. Creates Variables from annotations\n",
    "2. Builds ParametricCPDs using LazyConstructors\n",
    "3. Constructs the ProbabilisticModel with proper dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "008d0873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:03:55.452246Z",
     "start_time": "2025-11-14T10:03:55.447091Z"
    }
   },
   "source": [
    "# Create the encoder (input features -> embedding)\n",
    "encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x_train.shape[1], latent_dims), \n",
    "    torch.nn.LeakyReLU()\n",
    ")\n",
    "\n",
    "# Create the BipartiteModel\n",
    "concept_model = BipartiteModel(\n",
    "    task_names=task_names,\n",
    "    input_size=latent_dims,\n",
    "    annotations=annotations,\n",
    "    encoder=LazyConstructor(LinearZC),\n",
    "    predictor=LazyConstructor(LinearCC)\n",
    ")\n",
    "\n",
    "print(\"BipartiteModel structure:\")\n",
    "print(f\"  Task names: {task_names}\")\n",
    "print(f\"  Latent dimensions: {latent_dims}\")\n",
    "print(f\"  Concept propagator: {LinearZC.__name__}\")\n",
    "print(f\"  Task propagator: {LinearCC.__name__}\")\n",
    "print(f\"\\nUnderlying ProbabilisticModel:\")\n",
    "print(concept_model.probabilistic_model)\n",
    "print(f\"\\nThe model automatically created:\")\n",
    "print(f\"  - Variables for concepts and tasks\")\n",
    "print(f\"  - Encoder factors (embedding → concepts)\")\n",
    "print(f\"  - Predictor factors (concepts → tasks)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BipartiteModel structure:\n",
      "  Task names: ('xor',)\n",
      "  Latent dimensions: 10\n",
      "  Concept propagator: ProbEncoderFromEmb\n",
      "  Task propagator: ProbPredictor\n",
      "\n",
      "Underlying PGM:\n",
      "ProbabilisticGraphicalModel(\n",
      "  (factor_modules): ModuleDict(\n",
      "    (embedding): Identity()\n",
      "    (c1): ProbEncoderFromEmb(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=10, out_features=1, bias=True)\n",
      "        (1): Unflatten(dim=-1, unflattened_size=(1,))\n",
      "      )\n",
      "    )\n",
      "    (c2): ProbEncoderFromEmb(\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=10, out_features=1, bias=True)\n",
      "        (1): Unflatten(dim=-1, unflattened_size=(1,))\n",
      "      )\n",
      "    )\n",
      "    (xor): ProbPredictor(\n",
      "      (predictor): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "        (1): Unflatten(dim=-1, unflattened_size=(2,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "The model automatically created:\n",
      "  - Variables for concepts and tasks\n",
      "  - Encoder factors (embedding → concepts)\n",
      "  - Predictor factors (concepts → tasks)\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "id": "e2117604",
   "metadata": {},
   "source": [
    "## 5. Inference Engine\n",
    "\n",
    "We use the **DeterministicInference** engine on the BipartiteModel's underlying ProbabilisticModel:\n",
    "- **Evidence**: The embedding computed from input features\n",
    "- **Query**: The concepts and tasks we want to infer\n",
    "\n",
    "The BipartiteModel exposes its ProbabilisticModel via the `.probabilistic_model` attribute."
   ]
  },
  {
   "cell_type": "code",
   "id": "cb637558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:03:55.468992Z",
     "start_time": "2025-11-14T10:03:55.467047Z"
    }
   },
   "source": [
    "# Initialize the inference engine with the BipartiteModel's ProbabilisticModel\n",
    "inference_engine = DeterministicInference(concept_model.probabilistic_model)\n",
    "\n",
    "# Define the query (what we want to infer)\n",
    "query_concepts = [\"c1\", \"c2\", \"xor\"]\n",
    "\n",
    "print(\"Inference setup:\")\n",
    "print(f\"  Engine: DeterministicInference\")\n",
    "print(f\"  ProbabilisticModel source: concept_model.probabilistic_model\")\n",
    "print(f\"  Query variables: {query_concepts}\")\n",
    "print(f\"\\nInference flow:\")\n",
    "print(f\"  x_train → encoder → embedding → [c1, c2] → xor\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference setup:\n",
      "  Engine: DeterministicInference\n",
      "  PGM source: concept_model.pgm\n",
      "  Query variables: ['c1', 'c2', 'xor']\n",
      "\n",
      "Inference flow:\n",
      "  x_train → encoder → embedding → [c1, c2] → xor\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "id": "779aecb3",
   "metadata": {},
   "source": [
    "## 6. Complete Model Pipeline\n",
    "\n",
    "We combine the encoder and BipartiteModel into a complete pipeline:\n",
    "- **encoder**: Maps input features to latent embedding\n",
    "- **concept_model**: BipartiteModel that maps embedding to concepts and tasks\n",
    "\n",
    "This creates a Sequential model for easy training."
   ]
  },
  {
   "cell_type": "code",
   "id": "6070f489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:03:55.480623Z",
     "start_time": "2025-11-14T10:03:55.478038Z"
    }
   },
   "source": [
    "# Combine encoder and concept_model into a Sequential pipeline\n",
    "model = torch.nn.Sequential(encoder, concept_model)\n",
    "\n",
    "print(\"Complete model pipeline:\")\n",
    "print(model)\n",
    "print(f\"\\nPipeline structure:\")\n",
    "print(f\"  1. Encoder: {x_train.shape[1]} features → {latent_dims} dimensions\")\n",
    "print(f\"  2. BipartiteModel: {latent_dims} dimensions → concepts & tasks\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete model pipeline:\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (1): BipartiteModel(\n",
      "    (_encoder_builder): Propagator(\n",
      "      (module): ProbEncoderFromEmb(\n",
      "        (encoder): Sequential(\n",
      "          (0): Linear(in_features=10, out_features=1, bias=True)\n",
      "          (1): Unflatten(dim=-1, unflattened_size=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (_predictor_builder): Propagator(\n",
      "      (module): ProbPredictor(\n",
      "        (predictor): Sequential(\n",
      "          (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "          (1): Unflatten(dim=-1, unflattened_size=(2,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pgm): ProbabilisticGraphicalModel(\n",
      "      (factor_modules): ModuleDict(\n",
      "        (embedding): Identity()\n",
      "        (c1): ProbEncoderFromEmb(\n",
      "          (encoder): Sequential(\n",
      "            (0): Linear(in_features=10, out_features=1, bias=True)\n",
      "            (1): Unflatten(dim=-1, unflattened_size=(1,))\n",
      "          )\n",
      "        )\n",
      "        (c2): ProbEncoderFromEmb(\n",
      "          (encoder): Sequential(\n",
      "            (0): Linear(in_features=10, out_features=1, bias=True)\n",
      "            (1): Unflatten(dim=-1, unflattened_size=(1,))\n",
      "          )\n",
      "        )\n",
      "        (xor): ProbPredictor(\n",
      "          (predictor): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "            (1): Unflatten(dim=-1, unflattened_size=(2,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Pipeline structure:\n",
      "  1. Encoder: 2 features → 10 dimensions\n",
      "  2. BipartiteModel: 10 dimensions → concepts & tasks\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "054aa980",
   "metadata": {},
   "source": [
    "## 7. Training\n",
    "\n",
    "We train the complete model with a combined loss:\n",
    "- **Concept loss**: BCE loss between predicted and true concept labels (c1, c2)\n",
    "- **Task loss**: BCE loss between predicted and true task labels (xor)\n",
    "- **Total loss**: `concept_loss + concept_reg * task_loss`\n",
    "\n",
    "Training process:\n",
    "1. Compute embedding from input features\n",
    "2. Query the inference engine with the embedding as evidence\n",
    "3. Split predictions into concepts and tasks\n",
    "4. Compute losses and backpropagate"
   ]
  },
  {
   "cell_type": "code",
   "id": "f46cab9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:03:55.739431Z",
     "start_time": "2025-11-14T10:03:55.494308Z"
    }
   },
   "source": [
    "# Setup training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute embedding\n",
    "    emb = encoder(x_train)\n",
    "    \n",
    "    # Inference: query the ProbabilisticModel with embedding as evidence\n",
    "    cy_pred = inference_engine.query(query_concepts, evidence={'embedding': emb})\n",
    "    \n",
    "    # Split predictions: first columns are concepts, remaining are task\n",
    "    c_pred = cy_pred[:, :c_train.shape[1]]\n",
    "    y_pred = cy_pred[:, c_train.shape[1]:]\n",
    "\n",
    "    # Compute loss\n",
    "    concept_loss = loss_fn(c_pred, c_train)\n",
    "    task_loss = loss_fn(y_pred, y_train)\n",
    "    loss = concept_loss + concept_reg * task_loss\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log progress\n",
    "    if epoch % 100 == 0:\n",
    "        task_accuracy = accuracy_score(y_train, y_pred > 0.)\n",
    "        concept_accuracy = accuracy_score(c_train, c_pred > 0.)\n",
    "        print(f\"Epoch {epoch}: Loss {loss.item():.2f} | Task Acc: {task_accuracy:.2f} | Concept Acc: {concept_accuracy:.2f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 1.07 | Task Acc: 0.00 | Concept Acc: 0.22\n",
      "Epoch 100: Loss 0.52 | Task Acc: 0.09 | Concept Acc: 0.97\n",
      "Epoch 200: Loss 0.42 | Task Acc: 0.31 | Concept Acc: 0.99\n",
      "Epoch 300: Loss 0.40 | Task Acc: 0.32 | Concept Acc: 0.99\n",
      "Epoch 400: Loss 0.39 | Task Acc: 0.45 | Concept Acc: 0.99\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "1fc77ae8",
   "metadata": {},
   "source": [
    "## 8. Baseline Predictions (No Intervention)\n",
    "\n",
    "Let's examine the model's predictions without any interventions.\n",
    "The output contains concatenated predictions: [c1, c2, xor]"
   ]
  },
  {
   "cell_type": "code",
   "id": "e20d9c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:03:55.750279Z",
     "start_time": "2025-11-14T10:03:55.746429Z"
    }
   },
   "source": [
    "# Get baseline predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    emb = encoder(x_train)\n",
    "    cy_pred = inference_engine.query(query_concepts, evidence={'embedding': emb})\n",
    "\n",
    "print(\"Baseline predictions (first 5 samples):\")\n",
    "print(\"Format: [c1, c2, xor_class0, xor_class1]\")\n",
    "print(cy_pred[:5])\n",
    "print(f\"\\nShape: {cy_pred.shape}\")\n",
    "print(f\"  Columns 0-1: concept predictions (c1, c2)\")\n",
    "print(f\"  Columns 2-3: task predictions (xor one-hot)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline predictions (first 5 samples):\n",
      "Format: [c1, c2, xor_class0, xor_class1]\n",
      "tensor([[-5.2508e+00,  1.9481e+01,  1.0985e-01, -1.1001e-01],\n",
      "        [ 8.2998e+00,  4.2770e+00, -6.0167e-03,  7.3647e-03],\n",
      "        [-1.4043e+01, -1.3596e+01,  4.0784e-02, -4.3052e-02],\n",
      "        [-1.8641e+01,  1.6096e+01,  1.1045e-01, -1.1062e-01],\n",
      "        [ 4.7895e+00,  9.1838e+00, -4.1456e-03,  5.5098e-03]])\n",
      "\n",
      "Shape: torch.Size([1000, 4])\n",
      "  Columns 0-1: concept predictions (c1, c2)\n",
      "  Columns 2-3: task predictions (xor one-hot)\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "3bd5cfd0",
   "metadata": {},
   "source": [
    "## 9. Interventions in BipartiteModel\n",
    "\n",
    "The BipartiteModel framework supports interventions on the underlying ProbabilisticModel:\n",
    "- Access the ProbabilisticModel's factor modules via `concept_model.probabilistic_model.cpd_modules`\n",
    "- Apply interventions to specific factors (e.g., \"c1.encoder\")\n",
    "- Effects propagate through the graph structure\n",
    "\n",
    "### Intervention Setup:\n",
    "- **Policy**: RandomPolicy to randomly select samples and intervene on concept c1\n",
    "- **Strategy**: DoIntervention to set c1 to a constant value (-10)\n",
    "- **Layer**: Intervene at the \"c1.encoder\" factor in the ProbabilisticModel\n",
    "- **Quantile**: 1.0 (intervene on all selected samples)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f66dba23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:03:55.768043Z",
     "start_time": "2025-11-14T10:03:55.765203Z"
    }
   },
   "source": [
    "# Compute embedding for intervention\n",
    "emb = encoder(x_train)\n",
    "\n",
    "# Create annotations for intervention\n",
    "c_annotations = Annotations({1: AxisAnnotation([\"c1\"])})\n",
    "\n",
    "# Define intervention policy and strategy\n",
    "int_policy_c = UniformPolicy(\n",
    "    out_annotations=c_annotations,\n",
    "    subset=[\"c1\"]\n",
    ")\n",
    "int_strategy_c = DoIntervention(\n",
    "    model=concept_model.probabilistic_model.cpd_modules,\n",
    "    constants=-10\n",
    ")\n",
    "\n",
    "print(\"Intervention configuration:\")\n",
    "print(f\"  Policy: RandomPolicy on concept 'c1'\")\n",
    "print(f\"  Strategy: DoIntervention with constant value -10\")\n",
    "print(f\"  Target layer: c1.encoder (in BipartiteModel's ProbabilisticModel)\")\n",
    "print(f\"  Quantile: 1.0 (intervene on all selected samples)\")\n",
    "print(f\"\\nThis intervention will:\")\n",
    "print(f\"  1. Randomly select samples\")\n",
    "print(f\"  2. Set concept c1 to -10 for those samples\")\n",
    "print(f\"  3. Propagate the effect through the BipartiteModel to xor prediction\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervention configuration:\n",
      "  Policy: RandomPolicy on concept 'c1'\n",
      "  Strategy: DoIntervention with constant value -10\n",
      "  Target layer: c1.encoder (in BipartiteModel's PGM)\n",
      "  Quantile: 1.0 (intervene on all selected samples)\n",
      "\n",
      "This intervention will:\n",
      "  1. Randomly select samples\n",
      "  2. Set concept c1 to -10 for those samples\n",
      "  3. Propagate the effect through the BipartiteModel to xor prediction\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "id": "b9897f20",
   "metadata": {},
   "source": [
    "## 10. Applying the Intervention\n",
    "\n",
    "Now we apply the intervention and observe how the predictions change.\n",
    "Compare these results with the baseline predictions above to see the intervention's effect."
   ]
  },
  {
   "cell_type": "code",
   "id": "3640c2b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:03:55.782164Z",
     "start_time": "2025-11-14T10:03:55.776165Z"
    }
   },
   "source": [
    "print(\"Predictions with intervention:\")\n",
    "with intervention(\n",
    "    policies=[int_policy_c],\n",
    "    strategies=[int_strategy_c],\n",
    "    on_layers=[\"c1.encoder\"],\n",
    "    quantiles=[1]\n",
    "):\n",
    "    cy_pred_intervened = inference_engine.query(query_concepts, evidence={'embedding': emb})\n",
    "    print(\"Format: [c1, c2, xor_class0, xor_class1]\")\n",
    "    print(cy_pred_intervened[:5])\n",
    "\n",
    "print(\"\\nNote: Compare with baseline predictions above.\")\n",
    "print(\"You should see c1 values changed to -10 for randomly selected samples,\")\n",
    "print(\"and corresponding changes in the xor predictions.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with intervention:\n",
      "Format: [c1, c2, xor_class0, xor_class1]\n",
      "tensor([[-10.0000,  19.4812,   0.1104,  -0.1106],\n",
      "        [-10.0000,   4.2770,   0.1095,  -0.1097],\n",
      "        [-10.0000, -13.5958,   0.0408,  -0.0430],\n",
      "        [-10.0000,  16.0960,   0.1104,  -0.1106],\n",
      "        [-10.0000,   9.1838,   0.1104,  -0.1106]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Note: Compare with baseline predictions above.\n",
      "You should see c1 values changed to -10 for randomly selected samples,\n",
      "and corresponding changes in the xor predictions.\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "id": "0675f06a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored the BipartiteModel framework for concept-based learning:\n",
    "\n",
    "1. **Data**: Loaded the XOR toy dataset with binary concepts\n",
    "2. **Rich Annotations**: Defined metadata including distributions, types, and descriptions\n",
    "3. **BipartiteModel**: High-level abstraction that automatically builds a ProbabilisticModel\n",
    "4. **LazyConstructors**: Used to create encoder and predictor factors automatically\n",
    "5. **Inference**: Queried the underlying ProbabilisticModel for predictions\n",
    "6. **Training**: Trained with combined concept and task supervision\n",
    "7. **Interventions**: Applied causal interventions via the ProbabilisticModel structure\n",
    "\n",
    "### Key Advantages of BipartiteModel:\n",
    "- **High-level abstraction**: Simplified ProbabilisticModel construction from annotations\n",
    "- **Automatic structure**: Model builds Variables and ParametricCPDs automatically\n",
    "- **Rich metadata**: Support for distributions, cardinalities, and descriptions\n",
    "- **LazyConstructors**: Flexible way to specify encoder/predictor architectures\n",
    "- **ProbabilisticModel access**: Full access to underlying ProbabilisticModel for advanced operations\n",
    "- **Less boilerplate**: Reduces code needed compared to manual ProbabilisticModel construction\n",
    "\n",
    "### Comparison with Other Approaches:\n",
    "- **vs. Layer-based**: More structured, explicit graph representation\n",
    "- **vs. Manual ProbabilisticModel**: Less code, automatic construction from metadata\n",
    "- **Best for**: Production systems, complex models with many concepts/tasks\n",
    "\n",
    "This framework is ideal for:\n",
    "- Large-scale concept-based models with many variables\n",
    "- Systems requiring rich metadata for interpretability\n",
    "- Applications needing both ease-of-use and flexibility\n",
    "- Production deployments with complex concept hierarchies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptarium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
