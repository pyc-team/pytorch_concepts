{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eab3b24",
   "metadata": {},
   "source": [
    "# Probabilistic Model for Concept Bottleneck\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load and prepare data with concept annotations\n",
    "2. Define Variables and their probabilistic dependencies\n",
    "3. Build a Probabilistic Model (ProbabilisticModel) with ParametricCPDs\n",
    "4. Use inference engines to query the ProbabilisticModel\n",
    "5. Train the model with concept and task supervision\n",
    "6. Apply interventions to manipulate concept predictions in the ProbabilisticModel framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60858bb2",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "We import the necessary libraries:\n",
    "- **PyTorch**: for neural network building blocks and distributions\n",
    "- **sklearn**: for evaluation metrics\n",
    "- **torch_concepts**: for Variables, ParametricCPDs, ProbabilisticModel, and inference mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "id": "c00e0484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T09:19:00.488129Z",
     "start_time": "2025-11-17T09:19:00.484016Z"
    }
   },
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.distributions import Bernoulli, RelaxedOneHotCategorical\n",
    "\n",
    "from torch_concepts import Annotations, AxisAnnotation, Variable\n",
    "from torch_concepts.data import ToyDataset\n",
    "from torch_concepts.nn import (\n",
    "    LinearZC,\n",
    "    LinearCC,\n",
    "    ParametricCPD,\n",
    "    ProbabilisticModel,\n",
    "    RandomPolicy, \n",
    "    DoIntervention, \n",
    "    intervention, \n",
    "    DeterministicInference\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "e9309e7b",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "We load the XOR toy dataset and prepare the training data:\n",
    "- **Features (x_train)**: input features for the model\n",
    "- **Concepts (c_train)**: intermediate concept labels (binary: C1, C2)\n",
    "- **Targets (y_train)**: task labels (converted to one-hot encoding with 2 classes)\n",
    "- **Names**: concept and task attribute names"
   ]
  },
  {
   "cell_type": "code",
   "id": "1049685a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T09:19:00.501332Z",
     "start_time": "2025-11-17T09:19:00.496943Z"
    }
   },
   "source": [
    "# Hyperparameters\n",
    "latent_dims = 10\n",
    "n_epochs = 500\n",
    "n_samples = 1000\n",
    "concept_reg = 0.5\n",
    "\n",
    "# Load toy XOR dataset\n",
    "data = ToyDataset('xor', size=n_samples, random_state=42)\n",
    "x_train = data.data\n",
    "c_train = data.concept_labels\n",
    "y_train = data.target_labels\n",
    "concept_names = data.concept_attr_names\n",
    "task_names = data.task_attr_names\n",
    "\n",
    "# Convert y_train to one-hot encoding (2 classes)\n",
    "y_train = torch.cat([y_train, 1 - y_train], dim=1)\n",
    "\n",
    "# Define concept names for the ProbabilisticModel\n",
    "concept_names = ['c1', 'c2']\n",
    "\n",
    "print(f\"Dataset loaded:\")\n",
    "print(f\"  Features shape: {x_train.shape}\")\n",
    "print(f\"  Concepts shape: {c_train.shape}\")\n",
    "print(f\"  Targets shape: {y_train.shape}\")\n",
    "print(f\"  Concept names: {concept_names}\")\n",
    "print(f\"  Task name: xor\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded:\n",
      "  Features shape: torch.Size([1000, 2])\n",
      "  Concepts shape: torch.Size([1000, 2])\n",
      "  Targets shape: torch.Size([1000, 2])\n",
      "  Concept names: ['c1', 'c2']\n",
      "  Task name: xor\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "66b19a11",
   "metadata": {},
   "source": [
    "## 3. Variables: Defining the Graphical Structure\n",
    "\n",
    "In a Probabilistic Model, **Variables** represent random variables with:\n",
    "- **Name**: identifier for the variable\n",
    "- **Parents**: list of parent variables (defines the graph structure)\n",
    "- **Distribution**: probability distribution type (e.g., Bernoulli, Categorical)\n",
    "- **Size**: dimensionality of the variable\n",
    "\n",
    "We define:\n",
    "1. **input_var (emb)**: Latent embedding with no parents (root node)\n",
    "2. **concepts (c1, c2)**: Binary concepts that depend on the embedding\n",
    "3. **tasks (xor)**: Categorical task output that depends on the concepts\n",
    "\n",
    "This creates a graph: `emb → [c1, c2] → xor`"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the latent variable (embedding)\n",
    "input_var = Variable(\"input\", parents=[], size=latent_dims)\n",
    "\n",
    "# Define concept variables (depend on embedding)\n",
    "concepts = Variable(concept_names, parents=[\"input\"], distribution=Bernoulli)\n",
    "\n",
    "# Define task variable (depends on concepts)\n",
    "tasks = Variable(\"xor\", parents=concept_names, distribution=RelaxedOneHotCategorical, size=2)\n",
    "\n",
    "print(\"Variable structure:\")\n",
    "print(f\"\\nLatent variable:\")\n",
    "print(f\"  Name: {input_var.concepts}\")\n",
    "print(f\"  Parents: {input_var.parents}\")\n",
    "print(f\"  Size: {input_var.size}\")\n",
    "\n",
    "print(f\"\\nConcept variables:\")\n",
    "for i, c in enumerate(concepts):\n",
    "    print(f\"  Variable {i+1}:\")\n",
    "    print(f\"    Name: {c.concepts}\")\n",
    "    print(f\"    Parents: {c.parents}\")\n",
    "    print(f\"    Distribution: {c.distribution.__name__}\")\n",
    "    print(f\"    Size: {c.size}\")\n",
    "\n",
    "print(f\"\\nTask variable:\")\n",
    "print(f\"  Name: {tasks.concepts}\")\n",
    "print(f\"  Parents: {tasks.parents}\")\n",
    "print(f\"  Distribution: {tasks.distribution.__name__}\")\n",
    "print(f\"  Size: {tasks.size}\")"
   ],
   "id": "cd1b9a0643abd22c"
  },
  {
   "cell_type": "markdown",
   "id": "fcd125ad",
   "metadata": {},
   "source": [
    "## 4. ParametricCPDs: Neural Network Components\n",
    "\n",
    "**ParametricCPDs** are the computational units in the ProbabilisticModel that define the conditional probability distributions:\n",
    "- Each ParametricCPD takes parent variables as input and produces a child variable\n",
    "- ParametricCPDs are implemented as neural network modules\n",
    "\n",
    "We define three ParametricCPDs:\n",
    "1. **Backbone**: Maps input features to latent embedding (x → emb)\n",
    "2. **Concept Encoder**: Maps embedding to concept endogenous (emb → [c1, c2])\n",
    "3. **Task Predictor**: Maps concept endogenous to task predictions ([c1, c2] → xor)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ParametricCPD 1: Backbone (input features -> embedding)\n",
    "backbone = ParametricCPD(\n",
    "    \"input\",\n",
    "    parametrization=torch.nn.Sequential(\n",
    "        torch.nn.Linear(x_train.shape[1], latent_dims), \n",
    "        torch.nn.LeakyReLU()\n",
    "    )\n",
    ")\n",
    "\n",
    "# ParametricCPD 2: Concept encoder (embedding -> concepts)\n",
    "c_encoder = ParametricCPD(\n",
    "    [\"c1\", \"c2\"], \n",
    "    parametrization=LinearZC(\n",
    "        in_features=latent_dims,\n",
    "        out_features=concepts[0].size\n",
    "    )\n",
    ")\n",
    "\n",
    "# ParametricCPD 3: Task predictor (concepts -> task)\n",
    "y_predictor = ParametricCPD(\n",
    "    \"xor\", \n",
    "    parametrization=LinearCC(\n",
    "        in_features_endogenous=sum(c.size for c in concepts),\n",
    "        out_features=tasks.size\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ParametricCPD structure:\")\n",
    "print(f\"\\n1. Backbone ParametricCPD:\")\n",
    "print(f\"   Variable: emb\")\n",
    "print(f\"   Input size: {x_train.shape[1]}\")\n",
    "print(f\"   Output size: {latent_dims}\")\n",
    "\n",
    "print(f\"\\n2. Concept Encoder ParametricCPD:\")\n",
    "print(f\"   Variables: {['c1', 'c2']}\")\n",
    "print(f\"   Input: embedding of size {latent_dims}\")\n",
    "print(f\"   Output: concept endogenous of size {concepts[0].size}\")\n",
    "\n",
    "print(f\"\\n3. Task Predictor ParametricCPD:\")\n",
    "print(f\"   Variable: xor\")\n",
    "print(f\"   Input: concept endogenous of size {sum(c.size for c in concepts)}\")\n",
    "print(f\"   Output: task endogenous of size {tasks.size}\")"
   ],
   "id": "6d3ce58753d2ae77"
  },
  {
   "cell_type": "markdown",
   "id": "bc63417d",
   "metadata": {},
   "source": [
    "## 5. Probabilistic Model (ProbabilisticModel)\n",
    "\n",
    "The **ProbabilisticModel** combines Variables and ParametricCPDs into a coherent model:\n",
    "- It represents the joint probability distribution over all variables\n",
    "- It manages the computational graph defined by parent-child relationships\n",
    "- It provides an interface for inference and learning\n",
    "\n",
    "The ProbabilisticModel encapsulates:\n",
    "- All variables: latent, concepts, and tasks\n",
    "- All CPDs: backbone, concept encoder, and task predictor"
   ]
  },
  {
   "cell_type": "code",
   "id": "9af1acfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T09:19:00.566119Z",
     "start_time": "2025-11-17T09:19:00.563923Z"
    }
   },
   "source": [
    "# Initialize the Probabilistic Model\n",
    "concept_model = ProbabilisticModel(\n",
    "    variables=[input_var, *concepts, tasks],\n",
    "    parametric_cpds=[backbone, *c_encoder, y_predictor]\n",
    ")\n",
    "\n",
    "print(\"Probabilistic Model:\")\n",
    "print(concept_model)\n",
    "print(f\"\\nNumber of variables: {len(concept_model.variables)}\")\n",
    "print(f\"Variable names: {[v.concepts for v in concept_model.variables]}\")\n",
    "print(f\"\\nNumber of CPDs: {len(concept_model.parametric_cpds)}\")\n",
    "print(f\"\\nGraph structure:\")\n",
    "print(f\"  emb (latent) → [c1, c2] (concepts) → xor (task)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilistic Graphical Model:\n",
      "ProbabilisticGraphicalModel(\n",
      "  (factors): ModuleDict(\n",
      "    (emb): Factor(concepts=['emb'], module=Sequential)\n",
      "    (c1): Factor(concepts=['c1'], module=ProbEncoderFromEmb)\n",
      "    (c2): Factor(concepts=['c2'], module=ProbEncoderFromEmb)\n",
      "    (xor): Factor(concepts=['xor'], module=ProbPredictor)\n",
      "  )\n",
      ")\n",
      "\n",
      "Number of variables: 4\n",
      "Variable names: [['emb'], ['c1'], ['c2'], ['xor']]\n",
      "\n",
      "Number of factors: 4\n",
      "\n",
      "Graph structure:\n",
      "  emb (latent) → [c1, c2] (concepts) → xor (task)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "efe3a4ad",
   "metadata": {},
   "source": [
    "## 6. Inference Engine\n",
    "\n",
    "The **DeterministicInference** engine performs inference on the ProbabilisticModel:\n",
    "- **Evidence**: Known/observed variables (e.g., input features)\n",
    "- **Query**: Variables we want to predict\n",
    "- **Inference**: Forward pass through the graph to compute query variables\n",
    "\n",
    "We set up:\n",
    "- **Initial input**: The embedding variable (computed from x_train)\n",
    "- **Query concepts**: We want to infer c1, c2, and xor"
   ]
  },
  {
   "cell_type": "code",
   "id": "a993b44c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T09:19:00.590394Z",
     "start_time": "2025-11-17T09:19:00.588473Z"
    }
   },
   "source": [
    "# Initialize the inference engine\n",
    "inference_engine = DeterministicInference(concept_model)\n",
    "\n",
    "# Define the evidence (what we observe)\n",
    "initial_input = {'emb': x_train}\n",
    "\n",
    "# Define the query (what we want to infer)\n",
    "query_concepts = [\"c1\", \"c2\", \"xor\"]\n",
    "\n",
    "print(\"Inference setup:\")\n",
    "print(f\"  Engine: DeterministicInference\")\n",
    "print(f\"  Evidence variable: emb (from input features)\")\n",
    "print(f\"  Query variables: {query_concepts}\")\n",
    "print(f\"\\nInference will compute: x_train → emb → [c1, c2] → xor\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference setup:\n",
      "  Engine: DeterministicInference\n",
      "  Evidence variable: emb (from input features)\n",
      "  Query variables: ['c1', 'c2', 'xor']\n",
      "\n",
      "Inference will compute: x_train → emb → [c1, c2] → xor\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "1350e15d",
   "metadata": {},
   "source": [
    "## 7. Training\n",
    "\n",
    "We train the ProbabilisticModel with a combined loss:\n",
    "- **Concept loss**: BCE loss between predicted and true concept labels (c1, c2)\n",
    "- **Task loss**: BCE loss between predicted and true task labels (xor)\n",
    "- **Total loss**: `concept_loss + concept_reg * task_loss`\n",
    "\n",
    "During training:\n",
    "1. Query the inference engine to get predictions for c1, c2, and xor\n",
    "2. Split the output into concept and task predictions\n",
    "3. Compute losses and backpropagate through the entire ProbabilisticModel"
   ]
  },
  {
   "cell_type": "code",
   "id": "127b95f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T09:19:00.926214Z",
     "start_time": "2025-11-17T09:19:00.613696Z"
    }
   },
   "source": [
    "# Setup training\n",
    "optimizer = torch.optim.AdamW(concept_model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "concept_model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Inference: query the ProbabilisticModel for concept and task predictions\n",
    "    cy_pred = inference_engine.query(query_concepts, evidence=initial_input)\n",
    "    \n",
    "    # Split predictions: first columns are concepts, remaining are task\n",
    "    c_pred = cy_pred[:, :c_train.shape[1]]\n",
    "    y_pred = cy_pred[:, c_train.shape[1]:]\n",
    "\n",
    "    # Compute loss\n",
    "    concept_loss = loss_fn(c_pred, c_train)\n",
    "    task_loss = loss_fn(y_pred, y_train)\n",
    "    loss = concept_loss + concept_reg * task_loss\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log progress\n",
    "    if epoch % 100 == 0:\n",
    "        task_accuracy = accuracy_score(y_train, y_pred > 0.)\n",
    "        concept_accuracy = accuracy_score(c_train, c_pred > 0.)\n",
    "        print(f\"Epoch {epoch}: Loss {loss.item():.2f} | Task Acc: {task_accuracy:.2f} | Concept Acc: {concept_accuracy:.2f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 1.05 | Task Acc: 0.49 | Concept Acc: 0.25\n",
      "Epoch 100: Loss 0.51 | Task Acc: 0.02 | Concept Acc: 0.97\n",
      "Epoch 200: Loss 0.43 | Task Acc: 0.29 | Concept Acc: 0.98\n",
      "Epoch 300: Loss 0.41 | Task Acc: 0.31 | Concept Acc: 0.99\n",
      "Epoch 400: Loss 0.39 | Task Acc: 0.32 | Concept Acc: 0.99\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "f2b332fe",
   "metadata": {},
   "source": [
    "## 8. Baseline Predictions (No Intervention)\n",
    "\n",
    "Let's examine the model's predictions without any interventions.\n",
    "The output contains concatenated predictions: [c1, c2, xor]"
   ]
  },
  {
   "cell_type": "code",
   "id": "8210c55d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T09:19:00.935925Z",
     "start_time": "2025-11-17T09:19:00.931802Z"
    }
   },
   "source": [
    "# Get baseline predictions\n",
    "concept_model.eval()\n",
    "with torch.no_grad():\n",
    "    cy_pred = inference_engine.query(query_concepts, evidence=initial_input)\n",
    "\n",
    "print(\"Baseline predictions (first 5 samples):\")\n",
    "print(\"Format: [c1, c2, xor_class0, xor_class1]\")\n",
    "print(cy_pred[:5])\n",
    "print(f\"\\nShape: {cy_pred.shape}\")\n",
    "print(f\"  Columns 0-1: concept predictions (c1, c2)\")\n",
    "print(f\"  Columns 2-3: task predictions (xor one-hot)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline predictions (first 5 samples):\n",
      "Format: [c1, c2, xor_class0, xor_class1]\n",
      "tensor([[-3.8935e+00,  1.8834e+01,  1.0420e-01, -1.0441e-01],\n",
      "        [ 8.8618e+00,  4.0058e+00, -4.0338e-03,  5.3845e-03],\n",
      "        [-1.1902e+01, -1.3458e+01,  3.8285e-02, -4.0555e-02],\n",
      "        [-1.3823e+01,  1.3051e+01,  1.0638e-01, -1.0663e-01],\n",
      "        [ 4.0281e+00,  8.7874e+00, -9.3093e-04,  2.2892e-03]])\n",
      "\n",
      "Shape: torch.Size([1000, 4])\n",
      "  Columns 0-1: concept predictions (c1, c2)\n",
      "  Columns 2-3: task predictions (xor one-hot)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "fd9ad809",
   "metadata": {},
   "source": [
    "## 9. Interventions in ProbabilisticModel\n",
    "\n",
    "Interventions in the ProbabilisticModel framework work as follows:\n",
    "- We can set (do-operation) specific concept values\n",
    "- The effects propagate through the graph to downstream variables\n",
    "\n",
    "### Intervention Setup:\n",
    "- **Policy**: RandomPolicy to randomly select samples and intervene on concept c1\n",
    "- **Strategy**: DoIntervention to set c1 to a constant value (-10)\n",
    "- **Layer**: Intervene at the \"c1.encoder\" CPD\n",
    "- **Quantile**: 1.0 (intervene on all selected samples)"
   ]
  },
  {
   "cell_type": "code",
   "id": "05ec3334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T09:19:00.948344Z",
     "start_time": "2025-11-17T09:19:00.945946Z"
    }
   },
   "source": [
    "# Create annotations for intervention\n",
    "int_policy_c = RandomPolicy(out_features=concept_model.concept_to_variable[\"c1\"].size, scale=100)\n",
    "int_strategy_c = DoIntervention(model=concept_model.parametric_cpds, constants=-10)\n",
    "\n",
    "print(\"Intervention configuration:\")\n",
    "print(f\"  Policy: RandomPolicy on concept 'c1'\")\n",
    "print(f\"  Strategy: DoIntervention with constant value -10\")\n",
    "print(f\"  Target layer: c1.encoder\")\n",
    "print(f\"  Quantile: 1.0 (intervene on all selected samples)\")\n",
    "print(f\"\\nThis intervention will:\")\n",
    "print(f\"  1. Randomly select samples\")\n",
    "print(f\"  2. Set concept c1 to -10 for those samples\")\n",
    "print(f\"  3. Propagate the effect to the task prediction (xor)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervention configuration:\n",
      "  Policy: RandomPolicy on concept 'c1'\n",
      "  Strategy: DoIntervention with constant value -10\n",
      "  Target layer: c1.encoder\n",
      "  Quantile: 1.0 (intervene on all selected samples)\n",
      "\n",
      "This intervention will:\n",
      "  1. Randomly select samples\n",
      "  2. Set concept c1 to -10 for those samples\n",
      "  3. Propagate the effect to the task prediction (xor)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "e4357732",
   "metadata": {},
   "source": [
    "## 10. Applying the Intervention\n",
    "\n",
    "Now we apply the intervention and observe how the predictions change.\n",
    "Compare these results with the baseline predictions above to see the intervention's effect."
   ]
  },
  {
   "cell_type": "code",
   "id": "79a82395",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T09:19:01.018510Z",
     "start_time": "2025-11-17T09:19:01.014438Z"
    }
   },
   "source": [
    "print(\"Predictions with intervention:\")\n",
    "with intervention(policies=int_policy_c,\n",
    "                  strategies=int_strategy_c,\n",
    "                  target_concepts=[\"c1\", \"c2\"]):\n",
    "    cy_pred_intervened = inference_engine.query(query_concepts, evidence=initial_input)\n",
    "    print(\"Format: [c1, c2, xor_class0, xor_class1]\")\n",
    "    print(cy_pred_intervened[:5])\n",
    "\n",
    "print(\"\\nNote: Compare with baseline predictions above.\")\n",
    "print(\"You should see c1 values changed to -10 for randomly selected samples,\")\n",
    "print(\"and corresponding changes in the xor predictions.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with intervention:\n",
      "Format: [c1, c2, xor_class0, xor_class1]\n",
      "tensor([[-10.0000, -10.0000,   0.0383,  -0.0406],\n",
      "        [-10.0000, -10.0000,   0.0383,  -0.0406],\n",
      "        [-10.0000, -10.0000,   0.0383,  -0.0406],\n",
      "        [-10.0000, -10.0000,   0.0383,  -0.0406],\n",
      "        [-10.0000, -10.0000,   0.0383,  -0.0406]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Note: Compare with baseline predictions above.\n",
      "You should see c1 values changed to -10 for randomly selected samples,\n",
      "and corresponding changes in the xor predictions.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "f0fa5a78",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored Probabilistic Models for concept-based learning:\n",
    "\n",
    "1. **Data**: Loaded the XOR toy dataset with binary concepts\n",
    "2. **Variables**: Defined the graphical structure with latent, concept, and task variables\n",
    "3. **ParametricCPDs**: Created neural network components that compute conditional probabilities\n",
    "4. **ProbabilisticModel**: Combined variables and CPDs into a coherent probabilistic model\n",
    "5. **Inference**: Used deterministic inference to query the model\n",
    "6. **Training**: Trained with combined concept and task supervision\n",
    "7. **Interventions**: Applied causal interventions to manipulate concepts and observe effects\n",
    "\n",
    "### Key Advantages of ProbabilisticModel Framework:\n",
    "- **Explicit graph structure**: Clear representation of variable dependencies\n",
    "- **Probabilistic reasoning**: Each variable has an associated distribution\n",
    "- **Causal interventions**: Do-calculus operations for counterfactual analysis\n",
    "- **Modularity**: Easy to add/remove variables and CPDs\n",
    "- **Interpretability**: Graph structure makes the model's reasoning transparent\n",
    "\n",
    "This framework is particularly powerful for:\n",
    "- Causal reasoning and counterfactual analysis\n",
    "- Models with complex variable dependencies\n",
    "- Scenarios requiring explicit probabilistic modeling\n",
    "- Interpretable AI applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptarium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
