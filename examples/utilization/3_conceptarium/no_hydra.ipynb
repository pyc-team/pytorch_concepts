{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d348028",
   "metadata": {},
   "source": [
    "# Using Conceptarium Without Hydra\n",
    "\n",
    "This notebook demonstrates how to use the Conceptarium benchmarking tool without Hydra configuration files. \n",
    "\n",
    "**What you'll learn:**\n",
    "- Creating datasets with concept annotations\n",
    "- Instantiating a simple Concept Bottleneck Model (CBM)\n",
    "- Training with PyTorch Lightning\n",
    "- Making predictions on new data\n",
    "\n",
    "**Key objects:**\n",
    "- **Annotations**: Metadata describing your concepts (names, types, cardinalities)\n",
    "- **ConceptDataset**: PyTorch dataset wrapper for concept-based learning\n",
    "- **ConceptDataModule**: Lightning DataModule to handle data loading and splitting\n",
    "- **CBM**: our CBM model, implemented as a torch.nn.Module\n",
    "- **Predictor**: The LightningModule object build from the CBM model. The structure and functionalities of this LightningModule are shared across all models and datasets, used to ensure a unified engine that handles the full train/val/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae13cda",
   "metadata": {},
   "source": [
    "## 1. Setup Python Path\n",
    "\n",
    "Since `conceptarium` is not installed as a package, we add its parent directory to Python's search path.\n",
    "\n",
    "**Why this is needed:** The notebook is in `conceptarium/examples/`, but we need to import from `conceptarium/conceptarium/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path so we can import conceptarium\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the path to the parent directory (where conceptarium folder is)\n",
    "parent_path = Path.cwd().parent\n",
    "if str(parent_path) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_path))\n",
    "    \n",
    "print(f\"Added to path: {parent_path}\")\n",
    "print(f\"Python path: {sys.path[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985a964",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "**Core libraries:**\n",
    "- `torch`: PyTorch for neural networks\n",
    "- `pytorch_lightning`: Training framework\n",
    "\n",
    "**Conceptarium components:**\n",
    "- `Annotations`, `AxisAnnotation`: Describe concept structure\n",
    "- `ConceptDataset`: Dataset wrapper for concept data\n",
    "- `ConceptDataModule`: Handles train/val/test splits and dataloaders\n",
    "- `DeterministicInference`: Inference engine for the PGM\n",
    "- `CBM`: Concept Bottleneck Model\n",
    "- `Predictor`: Training engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# Conceptarium imports\n",
    "from torch_concepts import Annotations, AxisAnnotation\n",
    "from torch_concepts.data import ToyDataset\n",
    "from torch_concepts.data.base import ConceptDataset\n",
    "from torch_concepts.nn import DeterministicInference\n",
    "from conceptarium.data.base.datamodule import ConceptDataModule\n",
    "from conceptarium.nn.models.cbm import CBM\n",
    "from conceptarium.engines.predictor import Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16203561",
   "metadata": {},
   "source": [
    "## 3. Create Synthetic Dataset\n",
    "\n",
    "Generate a simple toy dataset to demonstrate the framework.\n",
    "\n",
    "**Dataset structure:**\n",
    "- **Inputs (X)**: 2-dimensional random features\n",
    "- **Concepts (C)**: 2 binary concepts derived from input features\n",
    "  - `concept_0`: 1 first feature > 0\n",
    "  - `concept_1`: 1 second feature > 0  \n",
    "- **Task (Y)**: Binary classification (XOR of the two concepts)\n",
    "\n",
    "**Note:** In Conceptarium, tasks are treated equally to concepts. Bboth names and values need to be concatenated. If an explicit separation of the task is needed by the model (as in the case of a standard CBM), this should (and will) be handled by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40fe99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic XOR dataset manually\n",
    "x = torch.rand(n_samples, 2)  # 2D random features in [0, 1]\n",
    "\n",
    "# Create binary concepts based on thresholds\n",
    "c1 = (x[:, 0] > 0.5).float().unsqueeze(1)  # concept_1: first feature > 0.5\n",
    "c2 = (x[:, 1] > 0.5).float().unsqueeze(1)  # concept_2: second feature > 0.5\n",
    "c = torch.cat([c1, c2], dim=1)\n",
    "\n",
    "# Create XOR task: y = c1 XOR c2\n",
    "y = (c1 != c2).float()\n",
    "\n",
    "concept_names_raw = ['concept_1', 'concept_2']\n",
    "task_names_raw = ['task_xor']\n",
    "\n",
    "# combine concept names into a single list\n",
    "concept_names = concept_names_raw + task_names_raw\n",
    "\n",
    "# same for data\n",
    "concepts = torch.concat([c, y], dim=1)\n",
    "\n",
    "print(f\"Dataset loaded:\")\n",
    "print(f\"  Features shape: {x.shape}\")\n",
    "print(f\"  Concepts shape: {concepts.shape}\")\n",
    "print(f\"  Concept names: {concept_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447708da",
   "metadata": {},
   "source": [
    "## 4. Define Annotations\n",
    "\n",
    "Annotations provide metadata about your concepts.\n",
    "\n",
    "**Required information:**\n",
    "- **labels**: Concept names (e.g., `['concept_0', 'concept_1', 'task_xor']`)\n",
    "- **metadata**: Dictionary with `type` for each concept (`'discrete'` or `'continuous'`)\n",
    "- **cardinalities**: Number of classes per concept (use `1` for binary concepts)\n",
    "\n",
    "**Key insight:** Cardinality of 1 means binary concept (optimized representation). Cardinality > 1 means multi-class categorical concept.\n",
    "\n",
    "**Annotations structure:**\n",
    "- Axis 0 (optional): Sample annotations\n",
    "- Axis 1 (required): Concept annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94dbdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concept names and task name\n",
    "# treating task as a concept\n",
    "concept_names = ['concept_1', 'concept_2', 'task_xor']\n",
    "\n",
    "# Create metadata for each concept/task\n",
    "metadata = {\n",
    "    'concept_1': {'type': 'discrete'},\n",
    "    'concept_2': {'type': 'discrete'},\n",
    "    'task_xor': {'type': 'discrete'},\n",
    "}\n",
    "\n",
    "# Cardinalities: use 1 for binary concepts/tasks (for optimization)\n",
    "cardinalities = (1, 1, 1)\n",
    "\n",
    "# Create AxisAnnotation for concepts\n",
    "concept_annotation = AxisAnnotation(\n",
    "    labels=concept_names,\n",
    "    metadata=metadata,\n",
    "    cardinalities=cardinalities\n",
    ")\n",
    "\n",
    "# Create full Annotations object.\n",
    "# Axis 0 for samples, if you need to annotate each sample separately\n",
    "# Axis 1 for concept annotations\n",
    "annotations = Annotations({\n",
    "    1: concept_annotation  # Concept axis\n",
    "})\n",
    "\n",
    "print(f\"Annotations created for {len(concept_names)} variables\")\n",
    "print(f\"All labels: {concept_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d37ec",
   "metadata": {},
   "source": [
    "## 5. Create ConceptDataset\n",
    "\n",
    "Wrap raw data and annotations into a PyTorch-compatible dataset.\n",
    "\n",
    "**Input format:**\n",
    "- `input_data`: Tensor of shape `(n_samples, n_features)`\n",
    "- `concepts`: Tensor of shape `(n_samples, n_concepts)` - includes both concepts and tasks\n",
    "- `annotations`: Annotations object from previous step\n",
    "\n",
    "**Output format** (what you get from `dataset[i]`):\n",
    "```python\n",
    "{\n",
    "    'inputs': {'x': tensor of shape (n_features,)},\n",
    "    'concepts': {'c': tensor of shape (n_concepts,)}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ConceptDataset\n",
    "dataset = ConceptDataset(\n",
    "    input_data=x,\n",
    "    concepts=concepts,\n",
    "    annotations=annotations\n",
    ")\n",
    "\n",
    "print(f\"Dataset created:\")\n",
    "print(f\"  Total samples: {len(dataset)}\")\n",
    "print(f\"  Sample structure: {list(dataset[0].keys())}\")\n",
    "print(f\"  Input shape: {dataset[0]['inputs']['x'].shape}\")\n",
    "print(f\"  Concepts shape: {dataset[0]['concepts']['c'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de179a29",
   "metadata": {},
   "source": [
    "## 6. Create DataModule\n",
    "\n",
    "DataModule handles data splitting and creates train/val/test dataloaders.\n",
    "\n",
    "**Key parameters:**\n",
    "- `val_size`, `test_size`: Fraction of data for validation and test (0.0-1.0)\n",
    "- `batch_size`: Number of samples per batch\n",
    "- `backbone`: Optional pretrained model for feature extraction (we use `None` for raw inputs)\n",
    "- `precompute_embs`: Whether to precompute embeddings with backbone and store them on disk.\n",
    "- `scalers`: Optional data normalization (not needed for discrete concepts)\n",
    "\n",
    "**After `setup('fit')`:** Dataset is split and ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataModule\n",
    "datamodule = ConceptDataModule(\n",
    "    dataset=dataset,\n",
    "    val_size=0.1,\n",
    "    test_size=0.2,\n",
    "    batch_size=32,\n",
    "    backbone=None,  # No pretrained backbone\n",
    "    precompute_embs=False, # No need to precompute embeddings with backbone\n",
    "    scalers=None,  # No scaling is needed for discrete concepts\n",
    "    workers=0\n",
    ")\n",
    "\n",
    "# Setup the data (split into train/val/test)\n",
    "datamodule.setup('fit')\n",
    "\n",
    "print(f\"DataModule created:\")\n",
    "print(f\"  Train samples: {datamodule.train_len}\")\n",
    "print(f\"  Val samples: {datamodule.val_len}\")\n",
    "print(f\"  Test samples: {datamodule.test_len}\")\n",
    "print(f\"  Batch size: {datamodule.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08818b6",
   "metadata": {},
   "source": [
    "## 7. Define Variable Distributions\n",
    "\n",
    "Specify which probability distributions to use for different concept types.\n",
    "\n",
    "**Distribution types:**\n",
    "- `discrete_card1`: For binary concepts (cardinality = 1)\n",
    "  - Uses `RelaxedBernoulli` for differentiable sampling\n",
    "- `discrete_cardn`: For multi-class concepts (cardinality > 1)\n",
    "  - Uses `RelaxedOneHotCategorical`\n",
    "- `continuous_card1/cardn`: For continuous concepts\n",
    "  - Uses `Delta` distribution (deterministic)\n",
    "\n",
    "**Temperature parameter:** Lower values (e.g., 0.1) make sampling closer to discrete/deterministic.\n",
    "\n",
    "**Note:** The model automatically selects the correct distribution based on each concept's cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable distributions map distribution types to their configurations\n",
    "# This tells the model which distribution to use for each type of concept\n",
    "# Here we define the distribution for binary concepts/tasks, as they all have cardinality 1\n",
    "variable_distributions = {\n",
    "    # For binary concepts (cardinality = 1)\n",
    "    'discrete_card1': {\n",
    "        'path': 'torch.distributions.RelaxedBernoulli',\n",
    "        'kwargs': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Variable distributions defined:\")\n",
    "for key, config in variable_distributions.items():\n",
    "    print(f\"  {key}: {config['path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9fe4c8",
   "metadata": {},
   "source": [
    "## 8. Create CBM Model\n",
    "\n",
    "Initialize a Concept Bottleneck Model.\n",
    "\n",
    "**Key parameters:**\n",
    "- `task_names`: Since a CBM separates concepts from task, provide list of task variable names (subset of concept labels).\n",
    "- `inference`: Inference engine class (e.g., `DeterministicInference`)\n",
    "- `input_size`: Dimensionality of input features\n",
    "- `annotations`: Concept metadata from step 4\n",
    "- `variable_distributions`: Distribution configs from step 7\n",
    "- `encoder_kwargs`: Kwargs of the encoder network.\n",
    "\n",
    "**Model architecture:**\n",
    "1. **Encoder**: Input → Embedding (MLP layers)\n",
    "2. **Model PGM**: Embedding → Concepts → Tasks\n",
    "\n",
    "**Note:** The model creates a Probabilistic Graphical Model (PGM) internally to represent concept relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42490214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task names (concepts that are predictions, not observations)\n",
    "task_names = ('task_xor',)\n",
    "\n",
    "# Create CBM model\n",
    "latent_dims = 64  # Hidden layer size in the encoder\n",
    "\n",
    "model = CBM(\n",
    "    task_names=task_names,\n",
    "    inference=DeterministicInference,\n",
    "    input_size=x.shape[1],\n",
    "    annotations=annotations,\n",
    "    variable_distributions=variable_distributions,\n",
    "    encoder_kwargs={'hidden_size': 16,\n",
    "                    'n_layers': 1,\n",
    "                    'activation': 'leaky_relu',\n",
    "                    'dropout': 0.}\n",
    ")\n",
    "\n",
    "print(f\"CBM model created:\")\n",
    "print(f\"  Input size: {x.shape[1]}\")\n",
    "print(f\" Encoder: {model.encoder}\")\n",
    "print(f\" Model PGM: {model.pgm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e73e5",
   "metadata": {},
   "source": [
    "## 9. Setup Loss Functions and Metrics\n",
    "\n",
    "Define how to compute loss and evaluate model performance.\n",
    "\n",
    "**Loss configuration:**\n",
    "- `discrete.binary`: Loss function for binary concepts\n",
    "  - `BCEWithLogitsLoss`: Binary cross-entropy for endogenous (includes sigmoid)\n",
    "\n",
    "**Metrics configuration:**\n",
    "- `discrete.binary.accuracy`: Accuracy metric for binary concepts\n",
    "  - `threshold: 0.0`: For logit inputs (since endogenous can be negative)\n",
    "\n",
    "**Format:** Each config specifies:\n",
    "- `path`: Full import path to the class\n",
    "- `kwargs`: Arguments to pass to the class constructor\n",
    "\n",
    "**Note:** The Predictor automatically applies the correct loss/metric based on concept type and cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f88fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss configuration\n",
    "loss_config = {\n",
    "    'discrete': {\n",
    "        'binary': {\n",
    "            'path': 'torch.nn.BCEWithLogitsLoss',\n",
    "            'kwargs': {}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Metrics configuration\n",
    "metrics_config = {\n",
    "    'discrete': {\n",
    "        'binary': {\n",
    "            'accuracy': {\n",
    "                'path': 'torchmetrics.classification.BinaryAccuracy',\n",
    "                'kwargs': {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Loss and metrics configured:\")\n",
    "print(f\"  Binary loss: {loss_config['discrete']['binary']['path']}\")\n",
    "print(f\"  Binary accuracy: {metrics_config['discrete']['binary']['accuracy']['path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7beaf",
   "metadata": {},
   "source": [
    "## 10. Create Predictor (Training Engine)\n",
    "\n",
    "The Predictor wraps the model and handles the training loop.\n",
    "\n",
    "**Key parameters:**\n",
    "- `model`: CBM model from step 8\n",
    "- `loss`, `metrics`: Configurations from step 9\n",
    "- `summary_metrics`: Compute metrics averaged across all concepts of each type\n",
    "- `perconcept_metrics`: Compute separate metrics for each individual concept. Also list of concepts names can be provided. 'True' abilitate it for all concepts\n",
    "- `optim_class`: Optimizer (e.g., `torch.optim.AdamW`)\n",
    "- `optim_kwargs`: Optimizer parameters (e.g., learning rate)\n",
    "- `scheduler_class`: Learning rate scheduler (optional)\n",
    "- `scheduler_kwargs`: Scheduler parameters (optional)\n",
    "\n",
    "**Trainer configuration:**\n",
    "- `max_epochs`: Maximum number of training epochs\n",
    "- `accelerator`: Hardware to use (`'auto'` detects GPU/CPU automatically)\n",
    "- `devices`: Number of GPUs/CPUs to use\n",
    "- `callbacks`: Training callbacks (e.g., `EarlyStopping` to stop when validation loss stops improving)\n",
    "\n",
    "**What it does:**\n",
    "- Computes forward pass and loss\n",
    "- Updates model parameters\n",
    "- Logs metrics to TensorBoard/WandB\n",
    "- Handles train/validation/test steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ffedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Predictor (PyTorch Lightning Module)\n",
    "engine = Predictor(\n",
    "    model=model,\n",
    "    loss=loss_config,\n",
    "    metrics=metrics_config,\n",
    "    preprocess_inputs=False, # whether to preprocess inputs (e.g., scaling)\n",
    "    scale_concepts=False, # whether to scale concepts before loss computation\n",
    "    summary_metrics=True, \n",
    "    perconcept_metrics=True,\n",
    "    optim_class=torch.optim.AdamW,\n",
    "    optim_kwargs={'lr': 0.0007},\n",
    "    scheduler_class=None,\n",
    "    scheduler_kwargs=None,\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=500,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "print(f\"Predictor and Trainer created:\")\n",
    "print(f\"Predictor: {engine}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214aedf4",
   "metadata": {},
   "source": [
    "## 11. Train the Model\n",
    "\n",
    "Use PyTorch Lightning Trainer for the training loop.\n",
    "\n",
    "**Training process:**\n",
    "1. For each epoch: train on all batches, validate on validation set\n",
    "2. Log metrics (loss, accuracy) for monitoring\n",
    "3. Stop early if validation loss doesn't improve for `patience` epochs\n",
    "4. Save best model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0dae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(engine, datamodule=datamodule)\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bad4e9",
   "metadata": {},
   "source": [
    "## 12. Test the Model\n",
    "\n",
    "Evaluate the trained model on the held-out test set.\n",
    "\n",
    "**What it does:**\n",
    "- Runs the model on all test batches\n",
    "- Computes test metrics (loss, accuracy)\n",
    "- Returns a dictionary with all test results\n",
    "\n",
    "**Interpreting results:**\n",
    "- `test_loss`: Average loss on test set\n",
    "- `test/SUMMARY-binary_accuracy`: Overall accuracy across all binary concepts\n",
    "- `test/concept_0_accuracy`, etc.: Per-concept accuracies (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_results = trainer.test(engine, datamodule=datamodule)\n",
    "\n",
    "print(\"\\nTest results:\")\n",
    "for key, value in test_results[0].items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27285f50",
   "metadata": {},
   "source": [
    "## 13. Make Predictions\n",
    "\n",
    "Use the trained model to make predictions on new data.\n",
    "\n",
    "**Prediction process:**\n",
    "1. Get a batch from the test dataloader\n",
    "2. Set model to evaluation mode (`engine.eval()`)\n",
    "3. Use `predict_batch()` to get model outputs\n",
    "4. Convert endogenous to probabilities with `torch.sigmoid()` (for binary concepts)\n",
    "\n",
    "**Output format:**\n",
    "- Raw predictions are **endogenous** (unbounded values)\n",
    "- Apply **sigmoid** to get probabilities in [0, 1]\n",
    "- For binary concepts: probability > 0.5 → class 1, else class 0\n",
    "\n",
    "**Comparing with ground truth:**\n",
    "- Predictions shape: `(batch_size, n_concepts)`\n",
    "- Ground truth shape: `(batch_size, n_concepts)`\n",
    "- Each column corresponds to one concept/task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test batch\n",
    "test_loader = datamodule.test_dataloader()\n",
    "batch = next(iter(test_loader))\n",
    "\n",
    "# Make predictions\n",
    "engine.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = engine.predict_batch(batch)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"\\nFirst 5 predictions (endogenous):\")\n",
    "print(predictions[:5])\n",
    "\n",
    "# Convert endogenous to probabilities\n",
    "probs = torch.sigmoid(predictions[:5])\n",
    "print(f\"\\nFirst 5 predictions (probabilities):\")\n",
    "print(probs)\n",
    "\n",
    "# Ground truth\n",
    "print(f\"\\nFirst 5 ground truth:\")\n",
    "print(batch['concepts']['c'][:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptarium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
