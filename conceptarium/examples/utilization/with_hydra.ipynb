{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a083ec42",
   "metadata": {},
   "source": [
    "## 1. Setup Python Path and Imports\n",
    "\n",
    "Add the parent directory to the Python path to import Conceptarium modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_path = Path.cwd().parent\n",
    "if str(parent_path) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_path))\n",
    "\n",
    "print(f\"Added to path: {parent_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59d1d5",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import Hydra and Conceptarium components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure warnings before importing third-party libraries\n",
    "import conceptarium.warnings_config  # noqa: F401\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from conceptarium.trainer import Trainer\n",
    "from conceptarium.hydra import parse_hyperparams\n",
    "from conceptarium.resolvers import register_custom_resolvers\n",
    "from conceptarium.utils import setup_run_env, clean_empty_configs, update_config_from_data\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fbbea4",
   "metadata": {},
   "source": [
    "## 3. Initialize Hydra and Load Configuration\n",
    "\n",
    "Use `hydra.initialize()` to set up Hydra in notebook mode, then compose the configuration.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7dcb9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from ../conf/sweep.yaml\n",
      "\n",
      "Dataset: asia\n",
      "Model: conceptarium.nn.models.cbm.CBM\n",
      "Max epochs: 500\n",
      "Batch size: 512\n",
      "\n",
      "============================================================\n",
      "Full Configuration:\n",
      "============================================================\n",
      "dataset:\n",
      "  batch_size: 512\n",
      "  val_size: 0.1\n",
      "  test_size: 0.2\n",
      "  ftune_size: 0.0\n",
      "  ftune_val_size: 0.0\n",
      "  concept_subset: null\n",
      "  n_gen: 10000\n",
      "  seed: ${seed}\n",
      "  backbone: null\n",
      "  precompute_embs: false\n",
      "  force_recompute: false\n",
      "  autoencoder_kwargs:\n",
      "    noise: 0.0\n",
      "    latent_dim: 32\n",
      "    lr: 0.0005\n",
      "    epochs: 2000\n",
      "    batch_size: 512\n",
      "    patience: 50\n",
      "  _target_: conceptarium.data.datamodules.bnlearn.BnLearnDataModule\n",
      "  name: asia\n",
      "  default_task_names:\n",
      "  - dysp\n",
      "  label_descriptions:\n",
      "    asia: a variable indicating whether a patient has recently been in Asia.\n",
      "    smoke: a variable indicating whether a patient is a smoker.\n",
      "    lung: a variable indicating whether a patient has lung cancer.\n",
      "    tub: a variable indicating whether a patient has tuberculosis.\n",
      "    bronc: a variable indicating whether a patient has bronchitis.\n",
      "    either: a variable indicating whether a patient has either tuberculosis or lung\n",
      "      cancer.\n",
      "    xray: a variable indicating whether a patient's chest X-ray shows abnormalities.\n",
      "    dysp: a variable indicating whether a patient has difficulty breathing (dyspnea).\n",
      "model:\n",
      "  encoder_kwargs:\n",
      "    hidden_size: 64\n",
      "    n_layers: 1\n",
      "    activation: leaky_relu\n",
      "    dropout: 0.2\n",
      "  variable_distributions:\n",
      "    discrete_card1:\n",
      "      path: torch.distributions.RelaxedBernoulli\n",
      "      kwargs:\n",
      "        temperature: 0.1\n",
      "    discrete_cardn:\n",
      "      path: torch.distributions.RelaxedOneHotCategorical\n",
      "      kwargs:\n",
      "        temperature: 0.1\n",
      "    continuous_card1:\n",
      "      path: torch_concepts.distributions.Delta\n",
      "    continuous_cardn:\n",
      "      path: torch_concepts.distributions.Delta\n",
      "  _target_: conceptarium.nn.models.cbm.CBM\n",
      "  task_names: ${dataset.default_task_names}\n",
      "  inference:\n",
      "    _target_: torch_concepts.nn.DeterministicInference\n",
      "    _partial_: true\n",
      "engine:\n",
      "  metrics:\n",
      "    discrete:\n",
      "      binary:\n",
      "        accuracy:\n",
      "          path: torchmetrics.classification.BinaryAccuracy\n",
      "          kwargs: {}\n",
      "      categorical:\n",
      "        accuracy:\n",
      "          path: torchmetrics.classification.MulticlassAccuracy\n",
      "          kwargs:\n",
      "            average: micro\n",
      "    continuous:\n",
      "      mae:\n",
      "        path: torchmetrics.regression.MeanAbsoluteError\n",
      "        kwargs: {}\n",
      "      mse:\n",
      "        path: torchmetrics.regression.MeanSquaredError\n",
      "        kwargs: {}\n",
      "  loss:\n",
      "    discrete:\n",
      "      binary:\n",
      "        path: torch.nn.BCEWithLogitsLoss\n",
      "        kwargs: {}\n",
      "      categorical:\n",
      "        path: torch.nn.CrossEntropyLoss\n",
      "        kwargs: {}\n",
      "    continuous:\n",
      "      path: torch.nn.MSELoss\n",
      "      kwargs: {}\n",
      "  _target_: conceptarium.engines.predictor.Predictor\n",
      "  optim_class:\n",
      "    _target_: hydra.utils.get_class\n",
      "    path: torch.optim.AdamW\n",
      "  optim_kwargs:\n",
      "    lr: 0.00075\n",
      "  enable_summary_metrics: true\n",
      "  enable_perconcept_metrics: true\n",
      "  preprocess_inputs: false\n",
      "  scale_concepts: false\n",
      "  train_interv_prob: 0.8\n",
      "  test_interv_policy: nodes_true\n",
      "  test_interv_noise: 0.0\n",
      "trainer:\n",
      "  max_epochs: 500\n",
      "  monitor: val_loss\n",
      "  patience: 30\n",
      "  logger: null\n",
      "  devices:\n",
      "  - 0\n",
      "seed: 42\n",
      "notes: test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_path = \"../conf\"\n",
    "config_name = \"sweep\"\n",
    "# Initialize Hydra with the configuration path\n",
    "with initialize(config_path=config_path, version_base=\"1.3\"):\n",
    "    # - Compose configuration\n",
    "    # - Override any parameters as needed\n",
    "    cfg = compose(config_name=config_name, \n",
    "                  overrides=['model=cbm', # any model\n",
    "                             'dataset=asia']) # any dataset\n",
    "\n",
    "print(f\"Configuration loaded from {config_path}/{config_name}.yaml\")\n",
    "print(f\"\\nDataset: {cfg.dataset.name}\")\n",
    "print(f\"Model: {cfg.model._target_}\")\n",
    "print(f\"Max epochs: {cfg.trainer.max_epochs}\")\n",
    "print(f\"Batch size: {cfg.dataset.batch_size}\\n\")\n",
    "\n",
    "# Print the full configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"Full Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d458ca",
   "metadata": {},
   "source": [
    "## 4. Setup Environment\n",
    "\n",
    "Configure random seeds and devices for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "124c4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seed, configure devices\n",
    "cfg = setup_run_env(cfg)  \n",
    "\n",
    "# Remove empty config entries. \n",
    "# Used for compatibility across models and datasets\n",
    "cfg = clean_empty_configs(cfg)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a8735",
   "metadata": {},
   "source": [
    "## 5. Instantiate Dataset (DataModule)\n",
    "\n",
    "Load and prepare the dataset. The datamodule handles:\n",
    "- Loading raw data (for the bnlearn datasets, the input data is extracted from the hidden representations of an autoencoder)\n",
    "- Creating annotations (concept metadata)\n",
    "- The setup method handle the dataset splitting into train/val/test\n",
    "- Creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37c959e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /home/gdefelice/.cache/conceptarium/asia\n",
      "Input shape: (10000, 32)\n",
      "Using raw input data without backbone preprocessing.\n",
      "\n",
      "  Total samples: 10000\n",
      "  Train: 7000, Val: 1000, Test: 2000\n",
      "  Batch size: 512\n",
      "  Concepts: ['asia', 'tub', 'smoke', 'lung', 'bronc', 'either', 'xray', 'dysp']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datamodule = instantiate(cfg.dataset, _convert_=\"all\")\n",
    "datamodule.setup('fit')\n",
    "\n",
    "print(f\"\\n  Total samples: {len(datamodule.dataset)}\")\n",
    "print(f\"  Train: {datamodule.train_len}, Val: {datamodule.val_len}, Test: {datamodule.test_len}\")\n",
    "print(f\"  Batch size: {datamodule.batch_size}\")\n",
    "print(f\"  Concepts: {list(datamodule.annotations.get_axis_labels(1))}\\n\")\n",
    "\n",
    "# Update config based on dataset properties\n",
    "cfg = update_config_from_data(cfg, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971e666",
   "metadata": {},
   "source": [
    "## 6. Instantiate Model\n",
    "\n",
    "Instantiate the model using hydra instantiation.\n",
    "\n",
    "Concept annotations and graph structure cannot be known before the dataset is instantiated.\n",
    "For this reason, we instantiate the model only partially with hydra, using the `_partial_` flag. The model is then completed by passing the dataset annotations and graph structure.\n",
    "\n",
    "- **annotations**: Concept metadata from dataset\n",
    "- **graph**: Structural dependencies between concepts (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "482f0fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model class: CBM\n",
      "  Model Encoder: MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Dense(\n",
      "      (affinity): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "  Model PGM: ProbabilisticGraphicalModel(\n",
      "  (factors): ModuleDict(\n",
      "    (embedding): Factor(concepts=['embedding'], module=Identity)\n",
      "    (asia): Factor(concepts=['asia'], module=ProbEncoderFromEmb)\n",
      "    (tub): Factor(concepts=['tub'], module=ProbEncoderFromEmb)\n",
      "    (smoke): Factor(concepts=['smoke'], module=ProbEncoderFromEmb)\n",
      "    (lung): Factor(concepts=['lung'], module=ProbEncoderFromEmb)\n",
      "    (bronc): Factor(concepts=['bronc'], module=ProbEncoderFromEmb)\n",
      "    (either): Factor(concepts=['either'], module=ProbEncoderFromEmb)\n",
      "    (xray): Factor(concepts=['xray'], module=ProbEncoderFromEmb)\n",
      "    (dysp): Factor(concepts=['dysp'], module=ProbPredictor)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = instantiate(cfg.model, _convert_=\"all\", _partial_=True)(annotations=datamodule.annotations,\n",
    "                                                                graph=datamodule.graph)\n",
    "\n",
    "print(f\"  Model class: {model.__class__.__name__}\")\n",
    "print(f\"  Model Encoder: {model.encoder}\")\n",
    "print(f\"  Model PGM: {model.pgm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6520b18",
   "metadata": {},
   "source": [
    "## 7. Instantiate Engine (Predictor)\n",
    "\n",
    "Instantiate the training engine using hydra.\n",
    "The engine wraps the model and handles:\n",
    "- **Loss computation**: From `engine/loss/*.yaml`\n",
    "- **Metrics computation**: From `engine/metrics/*.yaml`\n",
    "- **Optimization**: Optimizer and learning rate\n",
    "- **Training loops**: Train/validation/test steps\n",
    "\n",
    "Similarly to the model, the engine is instantiated partially with hydra using the `_partial_` flag, and then completed by passing the model instance.\n",
    "\n",
    "Finally, instantiate the PyTorch Lightning Trainer from the configuration. \n",
    "This define:\n",
    "- Early stopping (based on validation loss)\n",
    "- Model checkpointing (saves best model)\n",
    "- Logging (WandB/TensorBoard)\n",
    "- Progress bars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78fcbd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss configuration validated (all binary):\n",
      "  Binary (card=1): {'path': 'torch.nn.BCEWithLogitsLoss', 'kwargs': {}}\n",
      "  Categorical (card>1): unused\n",
      "  continuous: unused\n",
      "metrics configuration validated (all binary):\n",
      "  Binary (card=1): {'accuracy': {'path': 'torchmetrics.classification.BinaryAccuracy', 'kwargs': {}}}\n",
      "  Categorical (card>1): unused\n",
      "  continuous: unused\n"
     ]
    }
   ],
   "source": [
    "engine = instantiate(cfg.engine,  _convert_=\"all\", _partial_=True)(model=model)\n",
    "\n",
    "trainer = Trainer(cfg)\n",
    "trainer.logger.log_hyperparams(parse_hyperparams(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5c544",
   "metadata": {},
   "source": [
    "## 8. Train Model\n",
    "\n",
    "Train the PyTorch Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(engine, datamodule=datamodule)\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e59198",
   "metadata": {},
   "source": [
    "## 9. Test Model\n",
    "\n",
    "Evaluate the trained model on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aba498",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.test(datamodule=datamodule)\n",
    "trainer.logger.finalize(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26523e0",
   "metadata": {},
   "source": [
    "## 10. Make Predictions (Optional)\n",
    "\n",
    "Use the trained model to make predictions on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get a test batch\n",
    "test_loader = datamodule.test_dataloader()\n",
    "batch = next(iter(test_loader))\n",
    "\n",
    "print(batch)\n",
    "\n",
    "# Move engine to correct device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "engine = engine.to(device)\n",
    "\n",
    "# Make predictions\n",
    "engine.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = engine.predict_batch(batch)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"\\nFirst 5 predictions (logits):\")\n",
    "print(predictions[:5])\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = torch.sigmoid(predictions[:5])\n",
    "print(f\"\\nFirst 5 predictions (probabilities):\")\n",
    "print(probs)\n",
    "\n",
    "# Ground truth\n",
    "print(f\"\\nFirst 5 ground truth:\")\n",
    "print(batch['concepts']['c'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a15096e",
   "metadata": {},
   "source": [
    "## 11. Finalize and Cleanup\n",
    "\n",
    "Close the logger and finish the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize logger\n",
    "trainer.logger.experiment.finish()\n",
    "\n",
    "print(\"Experiment finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ba137",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "1. ✅ Load Hydra configuration in a notebook using `initialize()` and `compose()`. Eventually override configuration parameters\n",
    "2. ✅ Instantiate dataset, model, and engine from config\n",
    "3. ✅ Train and test a model using PyTorch Lightning\n",
    "4. ✅ Make predictions with the trained model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptarium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
