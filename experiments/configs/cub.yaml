# General experiment configuration
result_dir: results/cub
seeds: 1
load_results: true

# Dataset to be used
dataset_config:
  name: 'cub'
  root: '/homes/me466/data/CUB200/'
  training_augment: true
  val_proportion: 0.2

# The following config params will be shared across all runs
shared_params:
  # Training config
  epochs: 100
  batch_size: 64
  num_workers: 8
  check_val_every_n_epoch: 5
  log_every_n_steps: 25

  # Optimizer Config
  optimizer_config:
    name: sgd
    learning_rate: 0.01
    lr_scheduler_patience: 10
    lr_scheduler_factor: 0.1
    lr_scheduler_min_lr: 1E-5
    weight_decay: 0.000004
    momentum: 0.9

  # Early stopping
  early_stopping_config:
    monitor: loss
    patience: 5
    mode: min

  # Config of the actual encoder
  encoder_config:
    model: resnet18
    latent_dim: 32


  # Shared parameters across all runs in this experiment
  y_loss_fn: ce
  concept_weights: True  # Concepts are scaled based on their frequency
  latent_dim: 32
  class_reg: 1  # Weight of the task loss
  concept_reg: [10, 1, 0.1] # Weight of the concept loss. We will try all these values
  grid_variables:
    - concept_reg

# Here is where we indicate which runs we would like to include in this
# experiment
runs:
  - model_name: ConceptBottleneckModel
    run_name: CBM_cr_{concept_reg}

  - model_name: ConceptEmbeddingModel
    run_name: CEM_cr_{concept_reg}
    embedding_size: 32

  - model_name: ConceptResidualModel
    run_name: CRM_cr_{concept_reg}
    residual_size: 32

  - model_name: DeepConceptReasoning
    run_name: DCR_cr_{concept_reg}
    embedding_size: 32

  - model_name: LinearConceptEmbeddingModel
    run_name: LICEM_cr_{concept_reg}
    embedding_size: 32
    use_bias: true
    weight_reg: 0.0001
    bias_reg: 0.0001

  - model_name: ConceptMemoryReasoning
    run_name: CMR_cr_{concept_reg}
    embedding_size: 32
    memory_size: 20

  - model_name: "ConceptMemoryReasoning (embedding)"
    run_name: CMR_emb_cr_{concept_reg}
    embedding_size: 32
    memory_size: 20